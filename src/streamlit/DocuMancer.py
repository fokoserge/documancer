import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
from torchvision import transforms
import os
import gdown
import tensorflow as tf
import easyocr
from transformers import BertTokenizer
from transformers import TFBertForSequenceClassification, BertConfig

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Lambda
import os
from keras.saving import register_keras_serializable

import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import label_binarize
import json
from keras.models import load_model

import lime
import lime.lime_image
import lime.lime_text
from skimage.segmentation import mark_boundaries
import re
from tensorflow.keras.mixed_precision import set_global_policy

#-------------------------------------------------------------------------------
# Param√©trage de lancement
#-------------------------------------------------------------------------------
# Activer la pr√©cision mixte
set_global_policy('mixed_float16')

drive_model_BERT_dir = '/content/drive/MyDrive/formation Datascientest/jul24_bds_extraction/ETAPE 3/saved_BERTmodel_tensorflow_01'

model_path_multimodal = "/content/drive/MyDrive/formation Datascientest/jul24_bds_extraction/ETAPE 3/models/multimodal_model_complet_group_demo.keras"
model_path_cnn = "/content/drive/MyDrive/formation Datascientest/jul24_bds_extraction/CNN/saved_modelcnn.keras"
test_dataset_tf_path = "/content/drive/MyDrive/formation Datascientest/jul24_bds_extraction/ETAPE 3/models/merged_test_dataset_final_group"
image_path = "/content/drive/MyDrive/formation Datascientest/jul24_bds_extraction/_presentation/images"
name_app ='DocuMancer'
couleur_fond = '#2ED4DA'
couleur_police = '#382DD5'
#-------------------------------------------------------------------------------
# Param√©trage de lancement
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# Outillages et initialisation
#-------------------------------------------------------------------------------
@register_keras_serializable()
class TFBertLayer(tf.keras.layers.Layer):
    def __init__(self, bert_model_dir, **kwargs):
        # Enlever 'bert_model_dir' des kwargs pour √©viter de le passer √† la classe de base
        kwargs.pop('bert_model_dir', None)
        super(TFBertLayer, self).__init__(**kwargs)
        self.bert_model_dir = bert_model_dir
        # Charger la configuration avec output_hidden_states=True
        config = BertConfig.from_pretrained(self.bert_model_dir, output_hidden_states=True)
        self.bert_model = TFBertForSequenceClassification.from_pretrained(self.bert_model_dir, config=config)

    def call(self, inputs, training=False):
        input_ids = inputs[0]
        attention_mask = inputs[1]
        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask, training=training)
        hidden_states = outputs.hidden_states
        last_hidden_state = hidden_states[-1]
        cls_output = last_hidden_state[:, 0, :]
        return cls_output

    def get_config(self):
        config = super(TFBertLayer, self).get_config()
        config['bert_model_dir'] = self.bert_model_dir
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)

def load_model_multi(model_path):
    # Charger le mod√®le pr√©-entra√Æn√© avec Keras
    config = BertConfig.from_pretrained(drive_model_BERT_dir, output_hidden_states=True)
    # Charger le mod√®le
    model = load_model(model_path, custom_objects={'TFBertLayer': TFBertLayer})
    return model

def load_model_CNN(model_path):
    # Charger le mod√®le
    model = tf.keras.models.load_model(model_path)
    return model

def load_model_BERT(model_path):
    # Charger le mod√®le
    model = TFBertForSequenceClassification.from_pretrained(model_path)
    return model

def classify_image_text_keras(model, image_tensor, input_ids, attention_mask):
    # Obtenir les pr√©dictions du mod√®le
    output = model.predict([input_ids.numpy(), image_tensor.numpy(), attention_mask.numpy()])
    predicted_class = np.argmax(output, axis=1)[0]
    confidence_score = np.max(output) * 100
    return predicted_class, confidence_score

def classify_image_keras(model, image_tensor):
    # Obtenir les pr√©dictions du mod√®le
    output = model.predict([ image_tensor.numpy()])
    predicted_class = np.argmax(output, axis=1)[0]
    confidence_score = np.max(output) * 100
    return predicted_class, confidence_score

def classify_text_keras(model, input_ids, attention_mask):
    # Convertir input_ids et attention_mask en tenseurs
    input_ids_tensor = tf.convert_to_tensor(input_ids, dtype=tf.int32)
    attention_mask_tensor = tf.convert_to_tensor(attention_mask, dtype=tf.int32)

    # Obtenir la sortie du mod√®le
    output = model([input_ids_tensor, attention_mask_tensor])

    # V√©rifier si la sortie est un objet TensorFlow contenant des logits
    if isinstance(output, tf.Tensor):
        logits = output
    else:
        logits = output.logits

    # Utiliser np.argmax pour obtenir la classe pr√©dite
    predicted_class = np.argmax(logits, axis=-1)[0]  # axis=-1 pour la derni√®re dimension
    confidence_score = tf.nn.softmax(logits)[0].numpy().max() * 100

    return predicted_class, confidence_score

def download_model_from_drive(drive_url, output_path):
    gdown.download(drive_url, output_path, quiet=False)

#-------------------------------------------------------------------------------
# Outillages et initialisation
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# CSS
#-------------------------------------------------------------------------------
st.markdown("""
    <style>
    /* Styles pour le bouton de t√©l√©chargement */
    .stDownloadButton > button, .stDownloadButton > button * {
        background-color: #2ED4DA;
        color: #382DD5;
        border: none;
        padding: 2px 6px !importants;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        cursor: pointer;
        border-radius: 15px;
        font-family: Arial, sans-serif; /* Ajout de la police */
        font-weight: bold !important;
    }

    /* Styles pour les boutons LinkedIn */
    a.linkedin-button {
        display: inline-block; /* Changement de 'block' √† 'inline-block' */
        background-color: #2ED4DA !important;
        color: #382DD5 !important;
        font-weight: bold;
        padding: 2px 6px;
        text-decoration: none !important;
        border: none;
        border-radius: 15px;
        text-align: center;
        margin: 10px auto;
        width: 200px;
    }

    /* Appliquer les styles aux diff√©rents √©tats du lien */
    a.linkedin-button:link,
    a.linkedin-button:visited,
    a.linkedin-button:hover,
    a.linkedin-button:active {
        background-color: #2ED4DA !important;
        color: #382DD5 !important;
        text-decoration: none !important;
    }

    /* Styles pour la liste et les √©l√©ments de liste */
    ul.linkedin-list {
        list-style-type: none;
        padding: 0; /* Supprimer le padding par d√©faut */
        margin: 0;  /* Supprimer la marge par d√©faut */
        text-align: center; /* Centrer le contenu de la liste */
    }

    ul.linkedin-list li {
        display: block;
        margin-bottom: 2px; /* Ajouter de l'espace entre les boutons */
    }

    /* Justifier le texte des paragraphes */
    .content p, .content li {
        text-align: justify;
    }

    /* Styles pour le bloc de pr√©diction */
    .prediction-box {
        background-color: #2ED4DA;
        padding: 15px;
        border-radius: 10px;
        margin-top: 20px;
    }

    .prediction-box h3 {
        margin: 0;
        color: #382DD5;
        text-align: center;
    }

    .prediction-box h3 span {
        font-weight: bold;
    }

    .prediction-box p {
        font-size: 18px;
        margin: 5px 0;
        color: #382DD5;
        text-align: center;
    }

    .prediction-box p span {
        font-weight: bold;
    }

    /* Centrer l'image dans la fen√™tre modale en plein √©cran */
    div[role="dialog"] .stImage img {
        display: block;
        margin-left: auto;
        margin-right: auto;
    }

    </style>
    """, unsafe_allow_html=True)

# Ic√¥ne de baguette magique (vous pouvez choisir n'importe quelle ic√¥ne de Font Awesome ou une autre biblioth√®que d'ic√¥nes)
icon = "ü™Ñ"  # Ici c'est l'emoji "baguette magique", mais on pourrait aussi utiliser une ic√¥ne Font Awesome

# Cr√©er le titre avec style
title_html = f"""
    <h1 style='text-align: center; color: #2ED4DA;'>{name_app} {icon}</h1>
"""
#-------------------------------------------------------------------------------
# CSS
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# m√©thode principale
#-------------------------------------------------------------------------------
def main():
    #---------------------------------------------------------------------------
    # Sidebar
    #---------------------------------------------------------------------------
    image_datascientest = Image.open(image_path + '/datascientest.png')
    st.sidebar.image(image_datascientest, use_column_width=True)
    st.sidebar.markdown(title_html, unsafe_allow_html=True)

    page = st.sidebar.radio('Aller √†', [
        'Introduction au Projet',
        'Le dataset RVL-CDIP',
        'Strat√©gies d√©ploy√©es',
        'Pr√©diction par nos mod√®les',
        'Conclusion & perspectives'
    ])
    st.sidebar.markdown('<br>', unsafe_allow_html=True)
    st.sidebar.markdown(
        '''
        <div style="background-color: #0E1117; padding: 10px; border-radius: 5px; text-align: center;">
            <h3 style="margin-bottom: 20px;">Projet DS - Promotion Bootcamp √©t√© 2024</h3>
            <h3>Participants</h3>
            <ul class="linkedin-list">
                <li>
                    <a href="https://www.linkedin.com/in/kevin-ory" target="_blank" class="linkedin-button">Kevin Ory</a>
                </li>
                <li>
                    <a href="https://www.linkedin.com/in/kuate-foko-serge-ulrich" target="_blank" class="linkedin-button">Kuate Foko Serge Ulrich</a>
                </li>
                <li>
                    <a href="https://www.linkedin.com/in/nadir-ali-ahmed" target="_blank" class="linkedin-button">Nadir Ali Ahmed</a>
                </li>
                <li>
                    <a href="https://www.linkedin.com/in/xavier-truong-056837295" target="_blank" class="linkedin-button">Xavier Truong</a>
                </li>
            </ul>
        </div>
        ''',
        unsafe_allow_html=True
    )
    #---------------------------------------------------------------------------
    # Sidebar
    #---------------------------------------------------------------------------


    #---------------------------------------------------------------------------
    # Introduction au Projet
    #---------------------------------------------------------------------------
    if page == 'Introduction au Projet':
        # st.image(image_path + '/Documancer.gif', use_column_width=True)
        st.image(image_path + '/Documancer3.gif', use_column_width=True)
        st.title('pr√©sentation de '+ name_app)
        st.subheader('Introduction et Contexte du Projet')
        st.markdown('''
        Dans un monde o√π de nombreux secteurs comme la finance, la sant√©, et le juridique sont submerg√©s de documents physiques et num√©riques, l'automatisation de leur gestion est un enjeu crucial.
        Notre projet s'est concentr√© sur la classification automatique de documents en utilisant des techniques avanc√©es de deep learning.
        Le dataset que nous avons utilis√©, RVL-CDIP, contient 400 000 images de documents r√©partis en 16 cat√©gories (lettres, factures, m√©mos, etc.).
        ''')
        st.subheader('Objectifs du Projet')
        st.markdown('''
        L'objectif principal du projet est de d√©velopper un mod√®le performant capable de classer automatiquement les documents en fonction de leur type.
        Nous avons explor√© plusieurs approches, notamment les r√©seaux convolutifs (CNN) pour les images, BERT pour l'analyse textuelle apr√®s OCR, et une approche multimodale combinant les deux pour optimiser les performances.
        ''')

    #---------------------------------------------------------------------------
    # Le dataset RVL-CDIP
    #---------------------------------------------------------------------------
    elif page == 'Le dataset RVL-CDIP':

        st.title('Le dataset RVL-CDIP')
        st.markdown('''
        Le dataset **RVL-CDIP** (Ryerson Vision Lab Complex Document Information Processing) est une ressource de r√©f√©rence dans le domaine de la classification de documents.
        Il contient **400 000 images de documents num√©ris√©s**, r√©parties en **16 cat√©gories**, offrant une diversit√© qui permet de tester la capacit√© des mod√®les √† reconna√Ætre et diff√©rencier des types de documents vari√©s.
        ''')

        st.subheader('Pourquoi RVL-CDIP ?')
        st.markdown('''
        Ce dataset a √©t√© choisi pour sa richesse et sa diversit√©, mais aussi pour les d√©fis qu'il repr√©sente :
        - **Donn√©es vari√©es et h√©t√©rog√®nes** : Les documents proviennent de sources multiples, ce qui signifie que les mod√®les doivent √™tre capables de g√©rer une grande diversit√© de structures de documents.
        - **Conditions de num√©risation r√©alistes** : Les images ont √©t√© num√©ris√©es √† partir de documents papier avec des niveaux de bruit et de qualit√© tr√®s vari√©s, rendant le dataset proche des conditions r√©elles rencontr√©es en entreprise.
        - **Large volume de donn√©es** : Avec ses 400 000 images, RVL-CDIP permet de tester des mod√®les de deep learning √† grande √©chelle, ce qui est crucial pour obtenir des mod√®les robustes.
        ''')

        # Division en deux colonnes pour afficher le texte et le pie chart c√¥te √† c√¥te
        col1, col2 = st.columns([2, 1])  # La premi√®re colonne occupe 2/3, la seconde 1/3

        # Texte dans la colonne de gauche
        with col1:
            st.subheader('Structure des Donn√©es')
            st.markdown('''
            Le dataset est divis√© en trois ensembles pour faciliter l'entra√Ænement et l'√©valuation des mod√®les :
            - **Ensemble d'entra√Ænement** : 320 000 images utilis√©es pour ajuster les poids des mod√®les.
            - **Ensemble de validation** : 40 000 images permettant de v√©rifier la capacit√© du mod√®le √† g√©n√©raliser sur des donn√©es non vues.
            - **Ensemble de test** : 40 000 images pour √©valuer la performance finale du mod√®le.
            ''')

        # Pie chart dans la colonne de droite
        with col2:
            # Cr√©ation des donn√©es pour le pie chart
            labels = ['Entra√Ænement', 'Validation', 'Test']
            sizes = [320000, 40000, 40000]
            colors = ['#66b3ff', '#99ff99', '#ffcc99']
            explode = (0.1, 0, 0)  # Mettre en avant le segment de l'entra√Ænement

        # Cr√©ation du pie chart
        fig, ax = plt.subplots()
        ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
              shadow=True, startangle=90)
        ax.axis('equal')  # Assure que le pie chart est circulaire.

        # Afficher le graphique dans Streamlit
        st.pyplot(fig)

        st.markdown('''
        Chaque image est associ√©e √† une √©tiquette correspondant √† sa classe parmi les 16 cat√©gories suivantes :
        1. Lettre
        2. Formulaire
        3. Messagerie √©lectronique
        4. Manuscrit
        5. Publicit√©
        6. Rapport scientifique
        7. Publication scientifique
        8. Sp√©cification
        9. Dossier de fichiers
        10. Article de presse
        11. Budget
        12. Facture
        13. Pr√©sentation
        14. Questionnaire
        15. CV
        16. M√©mo
        ''')
        image_labels = Image.open(image_path + '/labels_example.png')
        st.image(image_labels, caption="Composition et exemples du dataset RVL-CDIP", use_column_width=True)

        st.subheader('Caract√©ristiques Techniques des Images')
        st.markdown('''
        - **Format** : Images en niveaux de gris, ce qui permet de r√©duire la complexit√© du traitement tout en conservant les informations essentielles.
        - **R√©solution** : Faible r√©solution √† **72 dpi**, ce qui ajoute un d√©fi pour la reconnaissance des d√©tails dans les documents.
        - **Qualit√©** : La qualit√© des images varie, avec des probl√®mes courants comme le bruit, les taches, les pliures, et les artefacts de num√©risation.
        ''')

        st.subheader('D√©fis et Enjeux')
        st.markdown('''
        L'utilisation du dataset RVL-CDIP ne vient pas sans d√©fis :
        - **Variabilit√© de qualit√©** : La qualit√© in√©gale des images rend difficile l'identification des caract√©ristiques visuelles des documents. Certains documents pr√©sentent des d√©fauts tels que des t√¢ches, des d√©formations, ou un faible contraste.
        - **Similarit√© entre classes** : Certaines cat√©gories de documents pr√©sentent des caract√©ristiques visuelles tr√®s proches (par exemple, les lettres et les m√©mos), ce qui complique la t√¢che de classification m√™me pour les mod√®les avanc√©s.
        - **Volume de donn√©es** : La gestion de 400 000 images demande une infrastructure de calcul adapt√©e, notamment pour le stockage, la pr√©visualisation et le traitement en batch lors de l'entra√Ænement des mod√®les.
        ''')

        st.subheader('Pourquoi ce Dataset est Essentiel ?')
        st.markdown('''
        Le dataset RVL-CDIP est un standard pour la recherche sur la classification de documents, ce qui permet de comparer les performances de nos mod√®les avec celles d'autres approches.
        En travaillant avec ce dataset, notre objectif est de d√©velopper des mod√®les capables de g√©n√©raliser √† partir de donn√©es r√©elles et de traiter efficacement les images de documents, m√™me dans des conditions difficiles.
        ''')

        # Titre principal de la pr√©sentation
        st.title("Pr√©sentation des Donn√©es du Projet")

        # Section 1 : Qualit√© des Images
        with st.expander("1. Qualit√© des Images"):
            st.markdown("""
            **Impact de la qualit√© des images :**
            La qualit√© des images (r√©solution, bruit, etc.) influence directement la capacit√© du mod√®le CNN √† extraire des caract√©ristiques pertinentes.

            **Probl√®mes identifi√©s :**
            Certaines images pr√©sentent des taches, des pliures ou des d√©formations, tandis que d'autres sont affect√©es par des probl√®mes de r√©solution ou de flou,
            rendant difficile l'extraction d'informations.
            """)
            # Ajout de l'image illustrant la qualit√© des images
            st.image(image_path + '/mauvaise_image_1.png', caption="Exemple de mauvaises Images", use_column_width=True)
            # Option de t√©l√©chargement de l'image
            with open(image_path + "/mauvaise_image_1.png", "rb") as img_file:
                st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="mauvaise_image_1.png", mime="image/png")

            # Ajout de l'image illustrant la qualit√© des images
            st.image(image_path + '/mauvaise_image_2.png', caption="Exemple de mauvaises Images", use_column_width=True)
            # Option de t√©l√©chargement de l'image
            with open(image_path + "/mauvaise_image_2.png", "rb") as img_file:
                st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="mauvaise_image_2.png", mime="image/png")

        # Section 2 : Erreurs de Labellisation
        with st.expander("2. Erreurs de Labelisation"):
            st.markdown("""
            **Ambigu√Øt√© dans la classification :**
            Certains documents peuvent appartenir √† plusieurs cat√©gories, ce qui complique l'√©tiquetage. Cette ambigu√Øt√© cr√©e des incoh√©rences dans
            les annotations et affecte la qualit√© des donn√©es.
            """)

            # Ajout de l'image illustrant les erreurs de labellisation
            st.image(image_path + '/erreur_de_label_1.png', caption="Erreurs de Labellisation", use_column_width=True)
            # Option de t√©l√©chargement de l'image
            with open(image_path + "/erreur_de_label_1.png", "rb") as img_file:
                st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="erreur_de_label_1.png", mime="image/png")

            # Ajout de l'image illustrant les erreurs de labellisation
            st.image(image_path + '/erreur_de_label_2.png', caption="Erreurs de Labellisation", use_column_width=True)
            # Option de t√©l√©chargement de l'image
            with open(image_path + "/erreur_de_label_2.png", "rb") as img_file:
                st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="erreur_de_label_2.png", mime="image/png")

            st.markdown("""
            **Erreurs de classification :**
            Les erreurs d'annotation dues √† des similitudes visuelles, √† l'erreur humaine ou √† la d√©finition personnelle du type de document
            affectent directement les performances des mod√®les, en particulier pour les classes qui partagent des caract√©ristiques communes.
            """)
            # Ajout de l'image illustrant les erreurs de labellisation
            st.image(image_path + '/error_accross_dataset.png', caption="Taux d'erreurs de Label par dataset", use_column_width=True)
            # Option de t√©l√©chargement de l'image
            with open(image_path + "/error_accross_dataset.png", "rb") as img_file:
                st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="error_accross_dataset.png", mime="image/png")

        # Section 3 : Types d'Images et Structure des Documents
        with st.expander("3. Types d'Images et Structure des Documents"):
            st.markdown("""
            **Vari√©t√© des Documents :**
            Le dataset contient diff√©rents types de documents, tels que des rapports, factures et correspondances, provenant de l'industrie
            du tabac. Ces documents ont des structures vari√©es, complexes et parfois simples. Cependant, le fait qu'ils appartiennent √† l'industrie
            du tabac pose un probl√®me de repr√©sentativit√© dans un contexte plus global o√π la structure de fond et de forme peut √™tre totalement diff√©rente.
            """)



    #---------------------------------------------------------------------------
    # Strat√©gies d√©ploy√©es
    #---------------------------------------------------------------------------
    elif page == 'Strat√©gies d√©ploy√©es':
        st.title('Strat√©gies d√©ploy√©es')
        # Ajout de la barre de s√©lection pour choisir le mod√®le
        image_strat = Image.open(image_path + '/strategies.png')
        st.image(image_strat, use_column_width=True)

        model_choice = st.selectbox(
            "Choisissez un mod√®le de d√©ploiement",
            ("CNN", "BERT", "BERT-CNN")
        )

        # Si le mod√®le s√©lectionn√© est CNN, affichage des informations d√©taill√©es
        if model_choice == "CNN":
            st.subheader("Mod√®le CNN : Classification de documents par les images")
            st.image(image_path + "/cnn_workflow.png", caption="Workflow CNN", use_column_width=True)

            st.markdown('''
            **Pourquoi avons-nous choisi CNN ?**
            - **Extraction des caract√©ristiques visuelles** : Les CNN sont id√©aux pour capturer des informations visuelles cl√©s telles que la structure de la mise en page et les formes des caract√®res. Cela les rend particuli√®rement efficaces pour distinguer diff√©rents types de documents (lettres, factures, m√©morandums).
            - **Robustesse face aux variations** : Gr√¢ce √† leur architecture, les CNN peuvent reconna√Ætre des motifs, peu importe leur position ou √©chelle dans l'image. Cela assure une classification fiable, m√™me lorsque la structure des documents varie.
            - **Traitement des images bruit√©es** : Les CNN sont performants m√™me avec des images de qualit√© variable (artefacts de num√©risation, flou). En utilisant plusieurs couches convolutives, ils filtrent les √©l√©ments non pertinents tout en conservant les informations essentielles.
            - **Transfer learning** : En exploitant des mod√®les pr√©-entra√Æn√©s comme VGG16 et ResNet, nous avons pu tirer parti de connaissances acquises pour r√©duire le temps d'entra√Ænement et am√©liorer la pr√©cision.
            - **Adaptabilit√© aux grands volumes** : Les CNN sont adapt√©s aux grands datasets comme RVL-CDIP (400 000 images), capables de g√©rer des millions de param√®tres tout en maintenant une performance stable et pr√©cise √† grande √©chelle.
            ''')
            st.markdown("**√âtapes suivies pour entra√Æner CNN :**")
            with st.expander("1. Pr√©traitement des images avec OpenCV"):
              st.code('''
                  image_resized = cv2.resize(image_np, (224, 224), interpolation=cv2.INTER_CUBIC)
                  gray = cv2.cvtColor(image_resized, cv2.COLOR_RGB2GRAY)
                  blurred = cv2.medianBlur(gray, 5)
                  _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
              ''', language='python')
              st.write("Redimensionnement des images √† 224x224 pixels et conversion en niveaux de gris pour simplifier l'image, suivi d'un flou m√©dian pour r√©duire le bruit.")
              # Ajout de l'image li√©e √† cette √©tape
              image_preprocessing = Image.open(image_path + '/cnn_pretraitement.png')
              st.image(image_preprocessing, caption="Pr√©traitement des images avec OpenCV", use_column_width=True)
              # Option de t√©l√©chargement de l'image
              with open(image_path + "/cnn_pretraitement.png", "rb") as img_file:
                  st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="pretraitement_opencv.png", mime="image/png")

            with st.expander("2. Pr√©paration des donn√©es pour TensorFlow"):
                st.code('''
                    # Conversion des images en objets TensorFlow Dataset
                    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))
                    dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)
                    dataset = dataset.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)
                ''', language='python')
                st.write("Cr√©ation de datasets TensorFlow pour un chargement plus rapide et un traitement en parall√®le, optimisant l'entra√Ænement du mod√®le.")

            with st.expander("3. Mod√®les et entra√Ænement avec et sans Transfer Learning"):
                st.markdown('''
                **Param√®tres d'entra√Ænement avec transfer learning:**
                - **Mod√®les pr√©-entra√Æn√©s** : VGG16 et ResNet50, adapt√©s pour la classification de documents √† partir de leurs images en niveaux de gris.
                - **Nombre d'√©poques** : 40 avec un taux d'apprentissage initial de 0.0001.
                ''')
                image_results = Image.open(image_path + '/cnn_w_vgg16.png')
                st.image(image_results, caption="Architecture mod√®le CNN avec vgg16", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/cnn_w_vgg16.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="cnn_w_vgg16.png", mime="image/png")

                st.markdown('''
                **Param√®tres d'entra√Ænement sans Transfer Learning:**
                - **Entra√Ænement** : Entra√Ænement de CNN personnalis√©s avec 4 couches convolutives.
                - **Taux de r√©gularisation (Dropout)** : 0.1 pour √©viter le surapprentissage.
                - **Nombre d'√©poques** : 40 avec un taux d'apprentissage initial de 0.0001.
                ''')
                image_results_2 = Image.open(image_path + '/cnn_wo_vgg16.png')
                st.image(image_results_2, caption="Architecture mod√®le CNN avec vgg16", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/cnn_wo_vgg16.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="cnn_wo_vgg16.png", mime="image/png")

                st.write("Les mod√®les pr√©-entra√Æn√©s ont converg√© plus rapidement et ont montr√© une meilleure capacit√© de g√©n√©ralisation sur le jeu de test.")

            with st.expander("4. R√©sultats de la Classification"):
                st.markdown('''
                **Pr√©cision sur les images non pr√©trait√©es :**
                - **VGG16 (Transfer Learning)** : 86% sur le jeu de test.
                - **ResNet50 (Transfer Learning)** : 85% sur le jeu de test.
                - **CNN entra√Æn√© depuis z√©ro** : 58% sur le jeu de test.
                ''')

                image_results_3 = Image.open(image_path + '/cnn_results_vgg16.png')
                st.image(image_results_3, caption="Comparaison des performances des diff√©rents mod√®les CNN", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/cnn_results_vgg16.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger les r√©sultats", data=img_file, file_name="cnn_results_vgg16.png", mime="image/png")

                image_results_4 = Image.open(image_path + '/cnn_results_ResNet50.png')
                st.image(image_results_4, caption="Comparaison des performances des diff√©rents mod√®les CNN", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/cnn_results_ResNet50.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger les r√©sultats", data=img_file, file_name="cnn_results_ResNet50.png", mime="image/png")

                image_results_5 = Image.open(image_path + '/cnn_results_zero.png')
                st.image(image_results_5, caption="Comparaison des performances des diff√©rents mod√®les CNN", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/cnn_results_zero.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger les r√©sultats", data=img_file, file_name="cnn_results_zero.png", mime="image/png")


                st.markdown('''
                **Pr√©cision sur les images pr√©trait√©es :**
                - **VGG16** : 59%, montrant une baisse de pr√©cision due √† la perte d'informations lors du pr√©traitement.
                - **CNN depuis z√©ro** : 34%, soulignant l'importance de la richesse des informations visuelles pour l'entra√Ænement.
                ''')
                image_results_6 = Image.open(image_path + '/cnn_results_processed_vgg16.png')
                st.image(image_results_6, caption="Comparaison des performances des diff√©rents mod√®les CNN", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/cnn_results_processed_vgg16.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger les r√©sultats", data=img_file, file_name="cnn_results_processed_vgg16.png", mime="image/png")

                image_results_7 = Image.open(image_path + '/cnn_results_processed_zero.png')
                st.image(image_results_7, caption="Comparaison des performances des diff√©rents mod√®les CNN", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/cnn_results_processed_zero.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger les r√©sultats", data=img_file, file_name="cnn_results_processed_zero.png", mime="image/png")

            with st.expander("5. R√©sum√© des R√©sultats CNN"):
                st.markdown('''
                            **Les mod√®les CNN bas√©s sur le transfer learning se sont r√©v√©l√©s les plus performants pour la classification des images de documents**, en particulier **VGG16**, qui a atteint une pr√©cision de **86%** sur les donn√©es test.
                            Bien que cette pr√©cision soit l√©g√®rement inf√©rieure aux meilleurs travaux sur le dataset RVL-CDIP, elle fournit une base solide pour la suite du projet.
                            Les approches de pr√©traitement n'ont pas apport√© d'am√©lioration significative et ont m√™me entra√Æn√© une d√©gradation des performances, renfor√ßant l'int√©r√™t de travailler avec les images originales.
                            ''')
                # Donn√©es pour le tableau des r√©sultats
                data = {
                    "Mod√®le": [
                        "CNN sans Transfer Learning",
                        "VGG16 (Transfer Learning)",
                        "CNN sans Transfer Learning",
                        "VGG16 (Transfer Learning)",
                        "ResNet50 (Transfer Learning)"
                        ],
                    "Pr√©traitement": [
                        "Pr√©trait√©",
                        "Pr√©trait√©",
                        "Non pr√©trait√©",
                        "Non pr√©trait√©",
                        "Non pr√©trait√©"
                        ],
                    "Accuracy sur Test": [
                        0.34,
                        0.59,
                        0.58,
                        0.86,
                        0.85
                        ]
                    }
                # Cr√©ation du DataFrame
                df = pd.DataFrame(data)
                # Arrondir les valeurs de 'Accuracy sur Test' √† deux d√©cimales
                df['Accuracy sur Test'] = df['Accuracy sur Test'].round(2)
                # Mise en √©vidence du mod√®le s√©lectionn√©
                highlight = lambda x: ['background-color: yellow' if (x['Mod√®le'] == "VGG16 (Transfer Learning)" and x['Pr√©traitement'] == "Non pr√©trait√©") else '' for i in x]
                # Affichage du tableau avec surlignage dans Streamlit
                st.subheader("Tableau des R√©sultats des Mod√®les CNN")
                st.dataframe(df.style.apply(highlight, axis=1))
                # Option de t√©l√©chargement du tableau en CSV
                @st.cache_data
                def convert_df_to_csv(df):
                    return df.to_csv(index=False).encode('utf-8')
                csv_data = convert_df_to_csv(df)
                st.download_button(
                    label="T√©l√©charger le tableau des r√©sultats en CSV",
                    data=csv_data,
                    file_name="resultats_cnn.csv",
                    mime="text/csv"
                    )
                st.markdown(
                    f"""
                    <div class="prediction-box">
                        <h3>
                            üéâ Mod√®le CNN : avec <span>27 millions</span> de param√®tres
                        </h3>
                        <h3>
                            Pr√©cision sur le dataset de test: <span>86%</span>
                        </h3>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

        # Si le mod√®le s√©lectionn√© est BERT, affichage des informations d√©taill√©es
        elif model_choice == "BERT":
            st.subheader("Mod√®le BERT : Classification de documents par les textes")
            st.image(image_path + "/bert.PNG", caption="√âtape finale de classification", use_column_width=True)
            st.markdown('''
            **Pourquoi avons-nous choisi BERT ?**
            - **Compr√©hension contextuelle bidirectionnelle** : BERT comprend le contexte des mots dans les deux sens, ce qui permet une interpr√©tation plus pr√©cise des phrases, essentiel pour la classification de documents.
            - **Pr√©-entra√Ænement sur de vastes corpus** : Gr√¢ce √† un pr√©-entra√Ænement sur des corpus comme Wikip√©dia, BERT s'adapte facilement √† des t√¢ches de classification de documents en capturant efficacement les relations entre les phrases.
            - **Efficacit√© sur les textes bruit√©s** : BERT fonctionne bien avec des textes issus de l'OCR, sans avoir besoin de traitements suppl√©mentaires comme la suppression des stop words.
            - **Performances sup√©rieures** : BERT surpasse de nombreux mod√®les pour la classification de documents et saisit des contextes complexes m√™me dans des textes longs.
            - **Robustesse face aux erreurs** : BERT est r√©sistant aux erreurs textuelles, essentiel pour les textes extraits de l'OCR.
            ''')
            st.markdown("**√âtapes suivies pour entra√Æner BERT :**")
            with st.expander("1. Pr√©traitement des images avec OpenCV"):
                st.code('''
                    image_resized = cv2.resize(image_np, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)
                    gray = cv2.cvtColor(image_resized, cv2.COLOR_RGB2GRAY)
                    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
                ''', language='python')
                st.write("Redimensionnement des images et conversion en niveaux de gris pour simplifier l'image et am√©liorer la lisibilit√© des petits caract√®res.")
                # Ajout de l'image li√©e √† cette √©tape
                image_preprocessing = Image.open(image_path + '/bert_pretraitement.PNG')
                st.image(image_preprocessing, caption="Pr√©traitement des images avec OpenCV", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/bert_pretraitement.PNG", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="pretraitement_opencv.png", mime="image/png")
            with st.expander("2. Application de l'OCR avec EasyOCR"):
                st.code('''
                    result = reader.readtext(image_np, detail=1)
                ''', language='python')
                st.write("Utilisation d'EasyOCR pour extraire le texte √† partir des images trait√©es.")
                # Ajout de l'image li√©e √† cette √©tape
                image_ocr = Image.open(image_path + '/bert_resultat_ocr.PNG')
                st.image(image_ocr, caption="Application de l'OCR avec EasyOCR", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/bert_resultat_ocr.PNG", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="ocr_easyocr.png", mime="image/png")
            with st.expander("3. Tokenisation avec le tokenizer de BERT"):
                st.code('''
                    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
                    tokens = tokenizer(text, max_length=512, truncation=True)
                ''', language='python')
                st.write("Le texte est segment√© en tokens avec un maximum de 512 tokens par s√©quence.")
            with st.expander("4. Entra√Ænement et √©valuation du mod√®le"):
              st.markdown('''
              **Param√®tres d'entra√Ænement de BERT** :
              - Mod√®le utilis√© : `bert-base-uncased` pour la classification avec 16 labels.
              - Taille de batch : 16
              - Nombre d'√©poques : 3
              - Taux de r√©gularisation (weight_decay) : 0.01
              - Longueur maximale des s√©quences : 512 tokens
              **Pr√©paration des donn√©es** :
              - Utilisation du tokenizer BERT avec padding et troncation pour uniformiser les s√©quences.
              - S√©paration en jeu d'entra√Ænement et de test (80% entra√Ænement, 20% test).
              ''')
              # Ajout des r√©sultats d'un premier √©chantillonnage
              st.markdown('''
              **√âchantillonnage initial** :
              - 24 000 images ont √©t√© utilis√©es pour un premier entra√Ænement de BERT.
              - **Performance sur 5 epochs :**
                - Accuracy : 77.85%
                - Pr√©cision : 78.37%
                - Rappel : 77.85%
                - F1-score : 77.99%
              ''')
              # Ajout de l'image li√©e aux r√©sultats issus de l'√©chantillon
              image_sample_result = Image.open(image_path + '/bert_echant_5epochs.PNG')
              st.image(image_sample_result, caption="R√©sultats issus de l'√©chantillon de 24 000 images", use_column_width=True)
              with open(image_path + "/bert_echant_5epochs.PNG", "rb") as img_file:
                  st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="resultats_echantillon.png", mime="image/png")
              st.markdown('''
              **Entra√Ænement complet sur le dataset** :
              - **Performance apr√®s 3 √©poques** :
                - Accuracy : 83.95%
                - Pr√©cision : 84.41%
                - Rappel : 83.95%
                - F1-score : 83.94%
              ''')
              # Ajout des images li√©es aux r√©sultats de l'entra√Ænement complet
              image_full_result_1 = Image.open(image_path + '/bert_datasetcomplet.PNG')
              st.image(image_full_result_1, caption="Matrice de confusion apr√®s traitement complet du dataset (Image 1)", use_column_width=True)
              with open(image_path + "/bert_datasetcomplet.PNG", "rb") as img_file:
                  st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="resultats_dataset_complet_1.png", mime="image/png")
              image_full_result_2 = Image.open(image_path + '/bert_datasetcomplet_scors.PNG')
              st.image(image_full_result_2, caption="R√©sultats apr√®s traitement complet du dataset (Image 2)", use_column_width=True)
              with open(image_path + "/bert_datasetcomplet_scors.PNG", "rb") as img_file:
                  st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="resultats_dataset_complet_2.png", mime="image/png")
              st.markdown(
                  f"""
                  <div class="prediction-box">
                      <h3>
                          üéâ Mod√®le BERT : avec <span>109 millions</span> de param√®tres
                      </h3>
                      <h3>
                          Pr√©cision sur le dataset de test: <span>84.41%</span>
                      </h3>
                  </div>
                  """,
                  unsafe_allow_html=True
              )
              
        elif model_choice == "BERT-CNN":
            st.subheader("Mod√®le BERT-CNN : Classification de documents par les images et les textes")
            st.markdown('''
            **Pourquoi avons-nous choisi un multimod√®le BERT-CNN ?**
            Le choix d'une architecture CNN-BERT se justifie par la compl√©mentarit√© de leurs forces pour traiter des donn√©es multimodales. 
            - **CNN (Convolutional Neural Network) :** est particuli√®rement efficace pour extraire des caract√©ristiques visuelles √† partir des images. 
            - **Tandis que BERT (Bidirectional Encoder Representations from Transformers) :** est excellent pour la compr√©hension des caract√©ristiques s√©mantiques du texte. 
            - **Leur combinaison** permet une meilleure repr√©sentation des documents multimodaux, am√©liorant ainsi la capacit√© du mod√®le √† comprendre des documents complexes de mani√®re holistique. Nous b√©n√©ficions d'une repr√©sentation enrichie qui tire parti des informations visuelles et textuelles, am√©liorant ainsi la pr√©cision et la capacit√© de classification, l√† o√π un mod√®le unique serait moins performant pour int√©grer ces deux types de donn√©es.
            ''')
            # Section 1 : Architecture du Mod√®le
            with st.expander("1. Architecture du Mod√®le"):
                st.markdown("""
                **Composantes du Mod√®le :**
                - **Mod√®le CNN** : Extraction des caract√©ristiques visuelles √† partir des images des documents.
                - **Mod√®le BERT** : Extraction des caract√©ristiques textuelles √† partir du texte extrait des documents.

                **Fusion des Caract√©ristiques :**
                - Les caract√©ristiques visuelles et textuelles sont concat√©n√©es pour former une repr√©sentation combin√©e du document.
                - Cette repr√©sentation est ensuite pass√©e √† travers des couches enti√®rement connect√©es pour la classification finale.
                """)
                # Ajout de l'image d'architecture du mod√®le
                st.image(image_path + '/architecture_sans_tuning.png', caption="architecture du Multimod√®le CNN-BERT sans tuning", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/architecture_sans_tuning.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="architecture_sans_tuning.png", mime="image/png")

            # Section 2 : Pr√©paration des Donn√©es
            with st.expander("2. Pr√©paration des Donn√©es"):
                st.markdown("""
                **Chargement des Donn√©es :**
                """)
                # Ajout de l'image de chargement des donn√©es
                st.image(image_path + '/chargement_de_donnees.png', caption="chargement de donn√©es du BERT", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/chargement_de_donnees.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="chargement_de_donnees.png", mime="image/png")

                st.markdown("""
                **Fusion des Donn√©es :**
                - Les datasets du CNN et de BERT sont fusionn√©s en utilisant l'ID unique des images pour associer correctement les caract√©ristiques visuelles et textuelles.
                """)
                # Ajout de l'image de fusion des donn√©es
                st.image(image_path + '/fusion_de_donnees.png', caption="fusion des donn√©es du CNN et OCR", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/fusion_de_donnees.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="fusion_de_donnees.png", mime="image/png")

                st.markdown("""
                **Pr√©traitement :**
                - **Pour le CNN** : Redimensionnement des images, normalisation, conversion en niveaux de gris.
                - **Pour BERT** : Tokenisation du texte, cr√©ation des `input_ids` et `attention_mask`.
                """)
                # Ajout de l'image de pr√©traitement des donn√©es
                st.image(image_path + '/pretraitement.png', caption="pr√©traitement des donn√©es d'entr√© du Multimod√®le", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/pretraitement.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="pretraitement.png", mime="image/png")

            # Section 3 : Construction et Entra√Ænement du Mod√®le
            with st.expander("3. Construction et Entra√Ænement du Mod√®le"):
                st.markdown("""
                **Mod√®le CNN :**
                - Utilisation d'un mod√®le pr√©-entra√Æn√© avec une couche dense de sortie pour les caract√©ristiques visuelles.
                - Les poids du CNN sont initialement gel√©s pour utiliser les caract√©ristiques pr√©-apprises.
                """)
                # Ajout de l'image du mod√®le CNN
                st.image(image_path + '/modele_CNN.png', caption="chargement du CNN pr√©-entra√Æn√©", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/modele_CNN.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="modele_CNN.png", mime="image/png")

                st.markdown("""
                **Mod√®le BERT :**
                - Utilisation de `TFBertForSequenceClassification` pour extraire les logits en tant que caract√©ristiques textuelles.
                - Les poids de BERT sont √©galement initialement gel√©s.
                """)
                # Ajout de l'image du mod√®le BERT
                st.image(image_path + '/modele_bert.png', caption="chargement du BERT pr√©-entra√Æn√©", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/modele_bert.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="modele_bert.png", mime="image/png")

                st.markdown("""
                **Couche Personnalis√©e pour BERT :**
                - Pour faire correspondre notre mod√®le BERT avec Tensorflow, une couche personnalis√©e est d√©finie pour utilis√© TFBertForSequenceClassification.
                """)
                # Ajout de l'image de Personnalisation de TFBERT
                st.image(image_path + '/Couche_Personnalisee_pour_BERT.png', caption="Personnalisation de TFBERT", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Couche_Personnalisee_pour_BERT.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Couche_Personnalisee_pour_BERT.png", mime="image/png")

                st.markdown("""
                **Combinaison des Caract√©ristiques et Construction du Mod√®le :**
                - Les sorties du CNN et de BERT sont concat√©n√©es.
                - Passage par des couches denses avec Dropout pour r√©duire le surapprentissage.
                """)
                # Ajout de l'image de combinaison des caract√©ristiques
                st.image(image_path + '/Combinaison_des_Caract√©ristiques_Construction_du_Modele.png', caption="Construction du Multimod√®le.png", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Combinaison_des_Caract√©ristiques_Construction_du_Modele.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Combinaison_des_Caract√©ristiques_Construction_du_Modele.png", mime="image/png")

                st.markdown("""
                **Entra√Ænement du Mod√®le :**
                - Compilation du mod√®le avec une fonction de perte adapt√©e (`sparse_categorical_crossentropy`) et un optimiseur (`Adam`).
                - Utilisation de callbacks comme `EarlyStopping` et `ReduceLROnPlateau` pour optimiser l'entra√Ænement.
                """)
                # Ajout de l'image de l'entra√Ænement du mod√®le
                st.image(image_path + '/Entrainement_du_Modele.png', caption="Entra√Ænement du Multimod√®le", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Entrainement_du_Modele.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Entrainement_du_Modele.png", mime="image/png")

            # Section 4 : R√©sultats du Mod√®le Multimodal
            with st.expander("4. R√©sultats du Mod√®le Multimodal"):
                st.markdown("**M√©triques Obtenues :**")
                # Ajout de l'image des r√©sultats
                st.image(image_path + '/confusion_matrix_1.png', caption="confusion matrix", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/confusion_matrix_1.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="confusion_matrix_1.png", mime="image/png")

                # Ajout de l'image des r√©sultats
                st.image(image_path + '/courbes_precision_multimodele_1.png', caption="courbes de pr√©cision du Multimod√®le", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/courbes_precision_multimodele_1.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="courbes_precision_multimodele_1.png", mime="image/png")

                # Ajout de l'image des r√©sultats
                st.image(image_path + '/Classification_report_1.png', caption="Classification report du Multimod√®le", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Classification_report_1.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Classification_report_1.png", mime="image/png")

                st.markdown("**Observations :**")
                st.markdown("""
                - Le mod√®le montre une bonne capacit√© √† classer les documents.
                - Cependant, des confusions persistent entre certaines classes similaires, notamment les classes 1, 12, et 13 et les classes 6 et 7.
                """)

            # Section 5 : Am√©liorations Apport√©es dans la Seconde Version
            with st.expander("5. Am√©liorations Apport√©es dans la Seconde Version"):
                st.markdown("""
                **Limitations de la Premi√®re Version :**
                """)
                st.markdown("""
                - **Confusion des Classes Similaires** : Difficult√© √† distinguer les classes avec des caract√©ristiques proches.
                """)
                st.markdown("""
                - **Absence de Fine-tuning** : Les mod√®les CNN et BERT √©tant gel√©s, le mod√®le ne peut pas s'adapter aux sp√©cificit√©s du dataset.
                """)

                st.markdown("""
                **Changements Apport√©s :**
                """)

                # Sous-partie : Regroupement des Classes
                st.markdown("""
                - **Regroupement des Classes** : Regroupement des classes pr√©sentant des taux √©lev√©s de confusion pour simplifier la t√¢che de classification.
                Par exemple, les classes 1, 12 et 13 sont regroup√©es en une seule classe.
                """)
                # Ajout de l'image associ√©e
                st.image(image_path + '/Regroupement_des_Classes.png', caption="Regroupement des classes", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Regroupement_des_Classes.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Regroupement_des_Classes.png", mime="image/png")

                # Sous-partie : Fine-tuning des Mod√®les
                st.markdown("""
                - **Fine-tuning des Mod√®les** : D√©gel des derni√®res couches du CNN et de BERT pour permettre un apprentissage plus adapt√© aux donn√©es sp√©cifiques.
                Cela permet au mod√®le d'apprendre des caract√©ristiques plus pertinentes pour notre dataset.
                """)
                # Ajout de l'image associ√©e
                st.image(image_path + '/Fine_tuning_des_Modeles.png', caption="Tuning du Multimod√®le", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Fine_tuning_des_Modeles.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Fine_tuning_des_Modeles.png", mime="image/png")

                # Sous-partie : Augmentation du Taux de Dropout
                st.markdown("""
                - **Augmentation du Taux de Dropout** : Le taux de Dropout est augment√© de 0.5 √† 0.6 pour am√©liorer la g√©n√©ralisation du mod√®le.
                """)
                # Ajout de l'image associ√©e
                st.image(image_path + '/Augmentation_Taux_Dropout.png', caption="Variation du Dropout", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Augmentation_Taux_Dropout.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Augmentation_Taux_Dropout.png", mime="image/png")

                # Sous-partie : Architecture du Multimod√®le avec tuning
                st.markdown("""
                - **Architecture du Multimod√®le avec tuning** : On peut observer la nouv√®le architecture du multimod√®le apr√®s le tuning.
                """)
                # Ajout de l'image de l'architecture
                st.image(image_path + '/architecture_avec_tuning.png', caption="Nouvelle architecture du Multimod√®le", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/architecture_avec_tuning.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="architecture_avec_tuning.png", mime="image/png")

                st.markdown("**M√©triques Obtenues :**")
                # Ajout de l'image des r√©sultats
                st.image(image_path + '/confusion_matrix_2.png', caption="confusion matrix", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/confusion_matrix_2.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="confusion_matrix_2.png", mime="image/png")

                # Ajout de l'image des r√©sultats
                st.image(image_path + '/courbes_pr√©cision_multimod√®les_2.png', caption="courbes de pr√©cision du Multimod√®le avec tuning", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/courbes_pr√©cision_multimod√®les_2.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="courbes_pr√©cision_multimod√®les_2.png", mime="image/png")

                # Ajout de l'image des r√©sultats
                st.image(image_path + '/Classification_report_2.png', caption="Classification report du Multimod√®le avec tuning", use_column_width=True)
                # Option de t√©l√©chargement de l'image
                with open(image_path + "/Classification_report_2.png", "rb") as img_file:
                    st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Classification_report_2.png.png", mime="image/png")

            # Section 6 : Comparaison des Performances
            with st.expander("6. Comparaison des Performances"):
                st.markdown("**Analyse Compar√©e**")
                st.table({
                    "Aspect": ["Validation Accuracy", "Test Accuracy", "Validation Loss", "Test Loss", "Confusion des Classes", "Fine-tuning", "Taux de Dropout"],
                    "Premi√®re Version": [0.8765, 0.8753, 0.4238, 0.4338, "√âlev√©e entre les classes 1, 12, 13", "Non", 0.5],
                    "Seconde Version": [0.8968, 0.8932, 0.4337, 0.4496, "R√©duite gr√¢ce au regroupement", "Oui", 0.6]
                })

                st.markdown(
                    f"""
                    <div class="prediction-box">
                        <h3>
                            üéâ Multimod√®le BERT-CNN : avec <span>28 millions</span> de param√®tres
                        </h3>
                        <h3>
                            Pr√©cision sur le dataset de test: <span>89.68%</span>
                        </h3>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

    #---------------------------------------------------------------------------
    # Pr√©diction par nos mod√®les
    #---------------------------------------------------------------------------
    elif page == 'Pr√©diction par nos mod√®les':
        st.image(image_path + '/Documancer.gif', use_column_width=True)
        st.title('Pr√©diction par nos mod√®les')
        st.markdown('''
        Les pr√©dictions ont √©t√© r√©alis√©es √† l'aide de plusieurs mod√®les d√©crits dans la strat√©gie d√©ploy√©e : CNN, BERT, et l'approche multimodale CNN-BERT.
        Vous pouvez visualiser ici les pr√©dictions effectu√©es par chaque mod√®le sur un √©chantillon de documents.
        ''')

        # Uploader un fichier image
        uploaded_file = st.file_uploader("Choisissez une image", type=['jpg', 'jpeg', 'png', 'tif', 'tiff', 'webp'])
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            st.image(image, caption='Image Import√©e', use_column_width=True)

            # Chemins des mod√®les
            model_paths = {
                'CNN': model_path_cnn,
                'BERT': drive_model_BERT_dir,
                'CNN-BERT': model_path_multimodal
            }


            # Ajout d'une option pour activer/d√©sactiver l'analyse LIME
            st.subheader("Options d'analyse")
            lime_enabled = st.checkbox("Effectuer l'analyse d'interpr√©tabilit√© avec LIME", value=False)

            # Choisir un mod√®le
            model_options = ['Choisissez le mod√®le !', 'CNN', 'BERT', 'CNN-BERT']
            model_choice = st.selectbox('Choisissez un mod√®le √† utiliser pour la classification', model_options, index=0)
            # V√©rifier que l'utilisateur a s√©lectionn√© un mod√®le
            if model_choice != 'Choisissez le mod√®le !':
                model_path = model_paths[model_choice]
                if not os.path.exists(model_path):
                    st.error("Le mod√®le n'a pas √©t√© trouv√©. Veuillez v√©rifier le chemin du mod√®le.")
                else:
                    # Charger le mod√®le choisi
                    if model_choice == 'CNN-BERT':
                        model_CNNBERT = load_model_multi(model_path)
                    elif model_choice == 'CNN':
                        model_CNN = load_model_CNN(model_path)
                    elif model_choice == 'BERT':
                        model_BERT = load_model_BERT(model_path)

                    # Extraction de texte si BERT ou CNN-BERT est choisi
                    if model_choice in ['CNN-BERT', 'BERT']:
                        st.subheader('Texte extrait via EasyOCR')
                        reader = easyocr.Reader(['en'])
                        extracted_text = reader.readtext(np.array(image), detail=0)
                        extracted_text = ' '.join(extracted_text)
                        st.text_area('Texte extrait via EasyOCR', extracted_text, height=200)

                        # Tokenisation avec BERT
                        st.subheader('Tokenisation du texte avec BERT')
                        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
                        tokens = tokenizer(
                            extracted_text,
                            padding='max_length',
                            truncation=True,
                            max_length=512,
                            return_tensors='tf'
                        )
                        input_ids = tokens['input_ids']
                        attention_mask = tokens['attention_mask']
                        # st.text(f"Tokens: {tokens['input_ids']}")

                        # R√©cup√©rer les tokens sous forme de liste
                        token_list = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])

                        # Filtrer les tokens pour ne garder que ceux qui nes token != '[PAD]']
                        filtered_tokens = [token for token in token_list if token != '[PAD]']

                        # Construire une cha√Æne de caract√®res HTML pour chaque token avec un style badge
                        token_html = ""
                        for token in filtered_tokens:
                            # Ajouter des couleurs altern√©es pour plus de lisibilit√©
                            color = "#2ED4DA" if token.startswith("##") else "#382DD5"
                            token_html += f'<span style="background-color: {color}; padding: 4px 8px; margin: 2px; border-radius: 5px; display: inline-block;">{token}</span> '

                        # Afficher le HTML avec Streamlit
                        st.markdown("#### Tokens g√©n√©r√©s :")
                        st.markdown(token_html, unsafe_allow_html=True)

                    # Pr√©parer l'image pour le mod√®le si CNN ou CNN-BERT est choisi
                    if model_choice in ['CNN-BERT', 'CNN']:
                        image = image.convert('L')  # Convertir en niveaux de gris
                        image = image.resize((224, 224))  # Redimensionner l'image √† la taille requise
                        image_tensor = np.expand_dims(np.array(image), axis=-1)  # Ajouter une dimension pour (224, 224, 1)
                        image_tensor = np.expand_dims(image_tensor, axis=0)  # Ajouter une autre dimension pour batch (1, 224, 224, 1)
                        image_tensor = tf.convert_to_tensor(image_tensor, dtype=tf.float32)

                        # Visualisation de l'image transform√©e
                        st.subheader('Image transform√©e')
                        fig, ax = plt.subplots()
                        ax.imshow(image_tensor.numpy().squeeze(), cmap='gray')
                        ax.axis('off')
                        st.pyplot(fig)

                    # Classification de l'image ou du texte
                    st.subheader("Classification de l'image")

                    if model_choice == 'CNN-BERT':
                        predicted_class, confidence_score = classify_image_text_keras(model_CNNBERT, image_tensor, input_ids, attention_mask)
                    elif model_choice == 'CNN':
                        predicted_class, confidence_score = classify_image_keras(model_CNN, image_tensor)
                    elif model_choice == 'BERT':
                        predicted_class, confidence_score = classify_text_keras(model_BERT, input_ids, attention_mask)

                    # Affichage du r√©sultat de la classification
                    class_labels = [
                        'Class 0: Letter', 'Class 1: Form, Presentation, Questionnaire', 'Class 2: Email', 'Class 3: Handwritten',
                        'Class 4: Advertisement', 'Class 5: Scientific Report & Publication',
                        'Class 6: Specification', 'Class 7: File Folder', 'Class 8: News Article',
                        'Class 9: Budget', 'Class 10: Invoice', 'Class 11: Resume', 'Class 12: Memo'
                    ]
                    # st.write(f'Cat√©gorie Pr√©dite : {class_labels[predicted_class]} avec une pr√©cision de {confidence_score:.2f}%')
                    # Affichage de la cat√©gorie pr√©dite avec un style visuel attrayant
                    st.markdown(
                        f"""
                        <div class="prediction-box">
                            <h3>
                                üéâ Cat√©gorie Pr√©dite : <span>{class_labels[predicted_class]}</span>
                            </h3>
                            <p>
                                Pr√©cision : <span>{confidence_score:.2f}%</span>
                            </p>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )

                    # Analyse avec LIME
                    if lime_enabled:
                      st.subheader("Analyse de l'interpr√©tabilit√© avec LIME")
                      if model_choice in ['CNN-BERT', 'CNN']:
                          explainer = lime.lime_image.LimeImageExplainer()

                          # Fonction de pr√©diction √† fournir √† LIME
                          def predict_fn(images):
                              images = np.expand_dims(images[:, :, :, 0], axis=-1)  # S'assurer que les images ont la bonne dimension
                              images = tf.convert_to_tensor(images, dtype=tf.float32)
                              if model_choice == 'CNN-BERT':
                                  batch_size = images.shape[0]
                                  input_ids_placeholder = tf.repeat(input_ids, batch_size, axis=0)  # R√©pliquer les tokens pour chaque image
                                  attention_mask_placeholder = tf.repeat(attention_mask, batch_size, axis=0)  # R√©pliquer les masques
                                  # Pr√©dire avec le mod√®le CNN-BERT en fournissant les images et les entr√©es textuelles
                                  return model_CNNBERT([ input_ids_placeholder, images, attention_mask_placeholder]).numpy()
                              elif model_choice == 'CNN':
                                  return model_CNN(images).numpy()

                          explanation = explainer.explain_instance(
                              image_tensor.numpy().squeeze(),
                              predict_fn,
                              top_labels=1,
                              hide_color=0,
                              num_samples=100
                          )

                          # Visualiser l'explication LIME
                          temp, mask = explanation.get_image_and_mask(
                              label=predicted_class,
                              positive_only=False,
                              num_features=10,
                              hide_rest=False
                          )
                          lime_image = mark_boundaries(temp, mask)
                          lime_image = lime_image.astype(np.float32) / 255.0
                          st.image(lime_image, caption='Explication LIME', use_column_width=True)

                          # Ajouter une description ade l'explication
                          st.markdown("""
                          L'explication ci-dessus montre les parties de l'image qui ont le plus influenc√© la classification.
                          Les zones mises en √©vidence indiquent les r√©gions qui ont contribu√© positivement ou n√©gativement √† la pr√©diction.
                          """)
                      if model_choice in ['BERT']:
                        explainer = lime.lime_text.LimeTextExplainer(class_names=class_labels)
                        explanation = explainer.explain_instance(
                            extracted_text,
                            lambda x: model_BERT.predict(tokenizer(x, padding='max_length', truncation=True, max_length=512, return_tensors='tf'))['logits'] if isinstance(model_BERT.predict(tokenizer(x, padding='max_length', truncation=True, max_length=512, return_tensors='tf')), dict) else model_BERT.predict(tokenizer(x, padding='max_length', truncation=True, max_length=512, return_tensors='tf')),
                            num_samples=100,
                            num_features=10
                        )
                        # Extraire l'HTML de l'explication LIME
                        raw_html = explanation.as_html()

                        # Personnalisation du HTML pour √©viter les chevauchements et am√©liorer la lisibilit√©
                        # Modifier les styles pour augmenter la lisibilit√©
                        modified_html = raw_html
                        modified_html = re.sub(r"font-size:\s*\d+px;", "font-size: 12px;", modified_html)  # R√©duire la taille de la police si n√©cessaire
                        modified_html = re.sub(r"width:\s*\d+px;", "width: auto;", modified_html)  # Ajuster la largeur automatiquement pour √©viter le d√©bordement
                        modified_html = re.sub(r"color:\s*[^;]+;", "color: black;", modified_html)  # Changer la couleur de la police en blanc
                        modified_html = re.sub(r"background-color:\s*[^;]+;", "background-color: #382DD5;", modified_html)  # Changer la couleur de fond pour correspondre √† la police
                        modified_html = re.sub(r"max-width:\s*\d+px;", "max-width: 100%;", modified_html)  # Augmenter la largeur maximale des conteneurs
                        # Ins√©rer une balise <style> globale au d√©but de l'HTML pour changer la couleur par d√©faut de la police
                        style_tag = """
                        <style>
                            body {
                                color: #382DD5 !important;
                                background-color: #2ED4DA !important;
                            }
                        </style>
                        """
                        modified_html = style_tag + modified_html

                        # Utiliser st.components.v1.html pour afficher le HTML modifi√©
                        st.markdown("### Explication LIME du texte")
                        st.components.v1.html(modified_html, height=600)

                        # Message d'explication
                        st.markdown("""
                        L'explication ci-dessus montre les mots du texte qui ont le plus influenc√© la classification.
                        Les mots avec des contributions positives ou n√©gatives sont affich√©s avec leur poids.
                        """)

                      if model_choice in ['CNN-BERT']:

                        def generate_dataset(texts, image):
                          # Pr√©parer l'image
                          if isinstance(image, Image.Image):
                              image = np.array(image)
                          if len(image.shape) == 2:  # Si l'image est en niveaux de gris
                              image = np.expand_dims(image, axis=-1)
                          image = tf.convert_to_tensor(image, dtype=tf.float16)
                          image = tf.expand_dims(image, axis=0)  # Ajouter la dimension du lot

                          # Cr√©er un Dataset TensorFlow pour charger les textes un par un
                          dataset = tf.data.Dataset.from_tensor_slices(texts)
                          dataset = dataset.map(lambda text: (
                              tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='tf')
                          ))
                          dataset = dataset.batch(1)
                          results = []
                          for inputs in dataset:
                              input_ids = inputs['input_ids']
                              attention_mask = inputs['attention_mask']
                              input_ids = tf.cast(input_ids, dtype=tf.float16)
                              attention_mask = tf.cast(attention_mask, dtype=tf.float16)
                              result = model_CNNBERT([input_ids, image, attention_mask]).numpy()
                              results.append(result)
                          return np.array(results)

                        def predict_text_batch(image, texts, batch_size=16):
                            # Convertir l'image PIL en tableau NumPy si n√©cessaire
                            if isinstance(image, Image.Image):
                                image = np.array(image)

                            # Ajouter la dimension des canaux si l'image est en niveaux de gris
                            if len(image.shape) == 2:  # Image en niveaux de gris sans dimension des canaux
                                image = np.expand_dims(image, axis=-1)

                            # Ajouter la dimension de lot pour l'image
                            image = np.expand_dims(image, axis=0)

                            # Convertir l'image en tenseur float16
                            image = tf.convert_to_tensor(image, dtype=tf.float16)

                            # Diviser les textes en batches
                            results = []
                            for i in range(0, len(texts), batch_size):
                                batch_texts = texts[i:i + batch_size]
                                # Tokenizer les textes avec une longueur maximale de 256 (ou ajuster selon la m√©moire disponible)
                                inputs = tokenizer(
                                    batch_texts, padding='max_length', truncation=True, max_length=512, return_tensors='tf'
                                )
                                input_ids = inputs['input_ids']
                                attention_mask = inputs['attention_mask']
                                # Convertir les entr√©es textuelles en float16 pour r√©duire la consommation de m√©moire
                                input_ids = tf.cast(input_ids, dtype=tf.float16)
                                attention_mask = tf.cast(attention_mask, dtype=tf.float16)
                                # R√©pliquer l'image pour correspondre √† la taille du batch
                                tiled_image = tf.tile(image, [len(batch_texts), 1, 1, 1])
                                # Passer l'image et les autres entr√©es au mod√®le
                                batch_results = model_CNNBERT([input_ids, tiled_image, attention_mask]).numpy()
                                results.extend(batch_results)
                            return np.array(results)

                        explainer = lime.lime_text.LimeTextExplainer(class_names=class_labels)
                        explanation = explainer.explain_instance(
                            extracted_text,
                            classifier_fn=lambda texts: predict_text_batch(image, texts, batch_size=16),
                            num_samples=100,  # R√©duire ou augmenter le nombre d'√©chantillons en fonction de la performance m√©moire
                            num_features=10
                        )
                        # Extraire l'HTML de l'explication LIME
                        raw_html = explanation.as_html()

                        # Personnalisation du HTML pour √©viter les chevauchements et am√©liorer la lisibilit√©
                        # Modifier les styles pour augmenter la lisibilit√©
                        modified_html = raw_html
                        modified_html = re.sub(r"font-size:\s*\d+px;", "font-size: 12px;", modified_html)  # R√©duire la taille de la police si n√©cessaire
                        modified_html = re.sub(r"width:\s*\d+px;", "width: auto;", modified_html)  # Ajuster la largeur automatiquement pour √©viter le d√©bordement
                        modified_html = re.sub(r"color:\s*[^;]+;", "color: black;", modified_html)  # Changer la couleur de la police en blanc
                        modified_html = re.sub(r"background-color:\s*[^;]+;", "background-color: white;", modified_html)  # Changer la couleur de fond pour correspondre √† la police
                        modified_html = re.sub(r"max-width:\s*\d+px;", "max-width: 100%;", modified_html)  # Augmenter la largeur maximale des conteneurs
                        # Ins√©rer une balise <style> globale au d√©but de l'HTML pour changer la couleur par d√©faut de la police
                        style_tag = """
                        <style>
                            body {
                                color: #382DD5 !important;
                                background-color: #2ED4DA !important;
                            }
                        </style>
                        """
                        modified_html = style_tag + modified_html

                        # Utiliser st.components.v1.html pour afficher le HTML modifi√©
                        st.markdown("### Explication LIME du texte")
                        st.components.v1.html(modified_html, height=600)

                        # Message d'explication
                        st.markdown("""
                        L'explication ci-dessus montre les mots du texte qui ont le plus influenc√© la classification.
                        Les mots avec des contributions positives ou n√©gatives sont affich√©s avec leur poids.
                        """)
    #---------------------------------------------------------------------------
    # Conclusion & perspectives
    #---------------------------------------------------------------------------
    elif page == 'Conclusion & perspectives':
        st.title('Conclusion & perspectives')
        st.markdown('''
        Le projet a d√©montr√© l'efficacit√© des mod√®les CNN et BERT pour la classification de documents.
        L'approche multimodale a surpass√© les performances des mod√®les individuels, atteignant une pr√©cision de **89%**, avec seulement  28,219,755 de param√®tres !.

        Perspectives d'am√©lioration :
        - **Augmentation des donn√©es** : Utiliser des techniques d'augmentation pour enrichir le dataset.
        - **Optimisation du pr√©traitement** : Am√©liorer le pr√©traitement des images et l'OCR.
        - **Optimisation des hyperparam√®tres** : Ajuster les hyperparam√®tres pour optimiser les performances.
        ''')
        st.markdown('<br>', unsafe_allow_html=True)
        st.markdown('<br>', unsafe_allow_html=True)
        st.markdown('<br>', unsafe_allow_html=True)
        st.markdown('<br>', unsafe_allow_html=True)
        st.markdown('<br>', unsafe_allow_html=True)
        st.markdown('<br>', unsafe_allow_html=True)
        image_sample_result = Image.open(image_path + '/Real_DocuMancer_parameters.png')
        st.image(image_sample_result, caption="R√©sultats mondiaux sur RVL-CDIP", use_column_width=True)
        with open(image_path + "/Real_DocuMancer_parameters.png", "rb") as img_file:
            st.download_button(label="T√©l√©charger l'image", data=img_file, file_name="Real_DocuMancer_parameters.png", mime="image/png")

#-------------------------------------------------------------------------------
# Lancement
#-------------------------------------------------------------------------------
if __name__ == "__main__":
    main()
