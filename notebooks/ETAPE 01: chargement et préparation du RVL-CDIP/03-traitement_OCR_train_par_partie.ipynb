{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1gyx1LO7WARDQw-Bg_uKVcFYM4NuwJ9WE","timestamp":1725052917438}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"386e2821361b4295aa3a0753c2fb20d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b200915b04a9478ba0875c30b6266473","IPY_MODEL_618c64c4a49f4349a47e22de0ef0843c","IPY_MODEL_a6beb9c5958a48f5bf0694f8bdb414f7"],"layout":"IPY_MODEL_5cfb469ad82b4440af0a9bf31cd2d711"}},"b200915b04a9478ba0875c30b6266473":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33b0f51acfa74208b177ced427cfd709","placeholder":"​","style":"IPY_MODEL_107da4f511e74e4a8ec63c96da343a90","value":"Filtrage des images: 100%"}},"618c64c4a49f4349a47e22de0ef0843c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7976338e5464c5eb022309f7972a100","max":32000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_173dafaf9768432c9a87b1ef34661345","value":32000}},"a6beb9c5958a48f5bf0694f8bdb414f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bf86f6d4a004315afb2548423c62048","placeholder":"​","style":"IPY_MODEL_cfac2e9b29a84eaea992903b48ab151b","value":" 32000/32000 [00:50&lt;00:00, 925.05image/s]"}},"5cfb469ad82b4440af0a9bf31cd2d711":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33b0f51acfa74208b177ced427cfd709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"107da4f511e74e4a8ec63c96da343a90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7976338e5464c5eb022309f7972a100":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"173dafaf9768432c9a87b1ef34661345":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bf86f6d4a004315afb2548423c62048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfac2e9b29a84eaea992903b48ab151b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0373707fa9b4c6ea6f0b2dbd1d7ac29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99415f8e4c274fb5b01aa1b640780902","IPY_MODEL_bc8d51250532440bb260f0ee24690b5f","IPY_MODEL_3cdb12990773416a8b2973a7328f29b2"],"layout":"IPY_MODEL_e70ef012560a4905ae667248431d2767"}},"99415f8e4c274fb5b01aa1b640780902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18b70ce3cb2242db95c50b015793c32f","placeholder":"​","style":"IPY_MODEL_6bd4126f646143c3947942339d98f2e2","value":"Prétraitement et OCR des images: 100%"}},"bc8d51250532440bb260f0ee24690b5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_087b0abcb72948dd85c40c452c02e30b","max":9795,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58bb763d8f0040cb9295cc60ae18fe23","value":9795}},"3cdb12990773416a8b2973a7328f29b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5429d877fa2047d4ad019a2199279902","placeholder":"​","style":"IPY_MODEL_52c9b791fab440ce963f456324f088ef","value":" 9795/9795 [2:15:40&lt;00:00,  1.25image/s]"}},"e70ef012560a4905ae667248431d2767":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18b70ce3cb2242db95c50b015793c32f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bd4126f646143c3947942339d98f2e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"087b0abcb72948dd85c40c452c02e30b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58bb763d8f0040cb9295cc60ae18fe23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5429d877fa2047d4ad019a2199279902":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52c9b791fab440ce963f456324f088ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_FxAfM9VVZU","executionInfo":{"status":"ok","timestamp":1726728285315,"user_tz":-120,"elapsed":31948,"user":{"displayName":"xav tru","userId":"01413854554802189079"}},"outputId":"4d51766d-a6f5-46a1-f265-d29ce4224642"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Etape 2 :\n","# Installer les bibliothèques nécessaires\n","!pip install easyocr\n","!pip install tensorflow\n","!pip install datasets\n","!pip install nltk\n","!pip install pandas\n","!pip install opencv-python-headless  # Pour éviter les conflits avec cv2"],"metadata":{"collapsed":true,"id":"aRZStT27ftdq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726737221376,"user_tz":-120,"elapsed":20838,"user":{"displayName":"SERGE FOKO","userId":"18212869974239608180"}},"outputId":"0d8ec702-1eaa-414a-c1de-afafbac3bcb4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting easyocr\n","  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.0+cu121)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.0+cu121)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (10.4.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.23.2)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.35.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.8.30)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n","Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n","Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n","Collecting datasets\n","  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"D-kWBeJSky3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#fabrication du dataset partiel"],"metadata":{"id":"lISTsg8plB5H"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from datasets import load_from_disk\n","from tqdm import tqdm  # Pour la barre de progression\n","\n","#-------------------------------------------------------------------------------\n","# Paramétrage de lancement\n","#-------------------------------------------------------------------------------\n","# Chemin pour trouver les datasets et enregistrer le CSV\n","results_dir = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/'\n","# Choix du dataset : 'val', 'train' ou 'test'\n","type_dataset = 'train'\n","\n","# Suffixe du dataset\n","dataset_suffixName = '_dataset_ID'\n","\n","train_csv_path = \"/content/drive/MyDrive/formation Datascientest/jul24_bds_extraction/ETAPE 3/notebooks/image_IDs_batch_8.csv\"\n","\n","# Chemin de sauvegarde du dataset réduit\n","reduced_train_dataset_path = \"/content/drive/MyDrive/formation Datascientest/RVL-CDIP/train_dataset_08\"\n","#-------------------------------------------------------------------------------\n","\n","# Charger le dataset\n","train_dataset = load_from_disk(os.path.join(results_dir, type_dataset + dataset_suffixName))\n","\n","# Charger les CSVs contenant les IDs échantillonnés\n","train_ids_df = pd.read_csv(train_csv_path)\n","\n","# Convertir les IDs en un set pour une recherche rapide\n","train_ids_set = set(train_ids_df['image_ID'])\n","\n","# Obtenir les indices des exemples à sélectionner avec une barre de progression\n","train_indices = [i for i, example in tqdm(enumerate(train_dataset), total=len(train_dataset), desc=\"Processing Dataset\")\n","                 if example['image_ID'] in train_ids_set]\n","\n","# Sélectionner les indices dans les datasets\n","reduced_train_dataset = train_dataset.select(train_indices)\n","\n","# Afficher la taille du dataset réduit\n","print(f\"Taille du reduced_train_dataset : {len(reduced_train_dataset)} exemples\")\n","\n","# Sauvegarder le dataset réduit\n","reduced_train_dataset.save_to_disk(reduced_train_dataset_path)\n","\n","print(\"Reduced datasets saved successfully!\")\n"],"metadata":{"id":"wh2yCq7FlGsM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Traitement OCR des images train\n","\n","\n","\n","\n"],"metadata":{"id":"bbvPQOLDfmYG"}},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/formation Datascientest/RVL-CDIP/ocr_train08_01_dataset_ID.csv\" \"/content/result\""],"metadata":{"id":"Whey3TlVCOwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464,"referenced_widgets":["386e2821361b4295aa3a0753c2fb20d6","b200915b04a9478ba0875c30b6266473","618c64c4a49f4349a47e22de0ef0843c","a6beb9c5958a48f5bf0694f8bdb414f7","5cfb469ad82b4440af0a9bf31cd2d711","33b0f51acfa74208b177ced427cfd709","107da4f511e74e4a8ec63c96da343a90","b7976338e5464c5eb022309f7972a100","173dafaf9768432c9a87b1ef34661345","1bf86f6d4a004315afb2548423c62048","cfac2e9b29a84eaea992903b48ab151b","b0373707fa9b4c6ea6f0b2dbd1d7ac29","99415f8e4c274fb5b01aa1b640780902","bc8d51250532440bb260f0ee24690b5f","3cdb12990773416a8b2973a7328f29b2","e70ef012560a4905ae667248431d2767","18b70ce3cb2242db95c50b015793c32f","6bd4126f646143c3947942339d98f2e2","087b0abcb72948dd85c40c452c02e30b","58bb763d8f0040cb9295cc60ae18fe23","5429d877fa2047d4ad019a2199279902","52c9b791fab440ce963f456324f088ef"]},"id":"__rZ2wU7fhRT","outputId":"27a9fc2c-fbaa-45b7-a0bf-f71626d1a6ee","executionInfo":{"status":"ok","timestamp":1726737104447,"user_tz":-120,"elapsed":989505,"user":{"displayName":"xav tru","userId":"01413854554802189079"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["GPU configuré pour l'utilisation.\n","Chargement du fichier CSV existant : /content/result/ocr_train08_01_dataset_ID.csv\n","début du filtrage des image\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"386e2821361b4295aa3a0753c2fb20d6","version_major":2,"version_minor":0},"text/plain":["Filtrage des images:   0%|          | 0/32000 [00:00<?, ?image/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Nombre d'images filtrées : 32000\n","Progress: |██████████████████████████████████████████████████| 100.0% Complete"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Progress: |██████████████████████████████████████████████████| 100.0% Complete"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n","/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=device))\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Nombre d'images déjà traitées : 22205\n","Début de l'OCR sur 32000 images\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n","  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0373707fa9b4c6ea6f0b2dbd1d7ac29","version_major":2,"version_minor":0},"text/plain":["Prétraitement et OCR des images:   0%|          | 0/9795 [00:00<?, ?image/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["Tous les résultats de l'OCR ont été enregistrés dans /content/result/ocr_train08_01_dataset_ID.csv\n","Temps de traitement total : 143.28 minutes\n"]}],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tqdm.notebook import tqdm\n","import cv2\n","import easyocr\n","import nltk\n","from PIL import Image, UnidentifiedImageError\n","from datasets import load_from_disk\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import io\n","import pandas as pd\n","import time\n","import threading\n","\n","#-------------------------------------------------------------------------------\n","#paramétrage de lancement\n","#-------------------------------------------------------------------------------\n","#chemin pour trouver les datasets en enregistrer le csv\n","results_dir = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/'\n","\n","# Nombre de threads pour le multithreading\n","num_threads = 16\n","#répertoire des OCR par image\n","temp_dir = '/content/tempreducedtrain/'\n","\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","# Chemin du fichier CSV pour sauvegarder les résultats OCR\n","csv_path = '/content/result/' +'ocr_train08_01_dataset_ID.csv'\n","\n","dataset_path = os.path.join(results_dir, 'train_dataset_08')\n","#-------------------------------------------------------------------------------\n","#-------------------------------------------------------------------------------\n","\n","start_time = time.time()\n","lock = threading.Lock()\n","\n","if os.path.exists(temp_dir):\n","    # Get the current time and format it\n","    current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","    # Rename the existing directory with the current time\n","    new_temp_dir = temp_dir[:-1] + \"_\" + current_time + '/'\n","    os.rename(temp_dir, new_temp_dir)\n","\n","# Create the temp directory\n","os.makedirs(temp_dir)\n","\n","# Configurer TensorFlow pour utiliser le GPU\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","if len(physical_devices) > 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","    print(\"GPU configuré pour l'utilisation.\")\n","\n","\n","\n","# Charger le DataFrame existant si le fichier CSV existe\n","if os.path.exists(csv_path):\n","    df_ocr = pd.read_csv(csv_path)\n","    print(f\"Chargement du fichier CSV existant : {csv_path}\")\n","else:\n","    # Initialiser un DataFrame vide si le fichier n'existe pas\n","    df_ocr = pd.DataFrame(columns=['image_ID', 'texte_de_ocr', 'confiance_de_ocr', 'label_de_image'])\n","    print(\"Aucun fichier CSV trouvé, création d'un nouveau DataFrame.\")\n","\n","# Chargement des datasets\n","dataset = load_from_disk(dataset_path)\n","\n","# Fonction pour filtrer les images du label\n","def filter_images(index):\n","    try:\n","        example = dataset[index]\n","        return example['image'], example['label'], example['image_ID']\n","    except Exception as e:\n","        print(f\"[WARNING] Erreur lors de la récupération de l'image à l'index {index}: {e}\")\n","    return None, None, None\n","\n","\n","\n","# Filtrer les indices pour le traitement\n","indices = list(range(len(dataset)))\n","\n","# Utiliser ThreadPoolExecutor pour le multithreading avec une barre de progression\n","print(f\"début du filtrage des image\")\n","label_invoice_images = []\n","with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","    futures = [executor.submit(filter_images, i) for i in indices]\n","    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Filtrage des images\", unit=\"image\"):\n","        result, label, image_ID= future.result()\n","        if result is not None:\n","            label_invoice_images.append((result, label, image_ID))\n","\n","print(f\"Nombre d'images filtrées : {len(label_invoice_images)}\")\n","\n","# Fonction de prétraitement de l'image avec OpenCV\n","def preprocess_image(image_bytes):\n","    try:\n","        # Charger l'image avec PIL pour gérer le format .tif\n","        image = Image.open(io.BytesIO(image_bytes))\n","        image = image.convert('RGB')  # Convertir en RGB pour compatibilité OpenCV\n","\n","        # Convertir l'image en un tableau NumPy\n","        image_np = np.array(image)\n","\n","        # Redimensionnement\n","        image_resized = cv2.resize(image_np, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)\n","\n","        # Conversion en niveaux de gris\n","        gray = cv2.cvtColor(image_resized, cv2.COLOR_RGB2GRAY)\n","\n","        # Seuillage adaptatif\n","        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n","\n","        # Suppression du bruit (médian)\n","        blur = cv2.medianBlur(thresh, 3)\n","\n","        # Encoder l'image traitée en bytes pour OCR\n","        _, buffer = cv2.imencode('.png', blur)\n","        preprocessed_image_bytes = buffer.tobytes()\n","\n","        return preprocessed_image_bytes\n","\n","    except Exception as e:\n","        print(f\"Erreur de prétraitement sur l'image: {e}\")\n","        return None\n","\n","# Initialiser le lecteur EasyOCR\n","reader = easyocr.Reader(['en'], gpu=True)\n","\n","# Fonction pour appliquer l'OCR avec EasyOCR\n","def apply_ocr(image_bytes):\n","    try:\n","        # Convertir les bytes en une image pour EasyOCR\n","        image_np = np.array(Image.open(io.BytesIO(image_bytes)))\n","        result = reader.readtext(image_np, detail=1)\n","\n","        # Préparer les données à retourner\n","        texts = []\n","        confidences = []\n","        coordinates = []\n","        for (coord, text, confidence) in result:\n","            texts.append(text)\n","            confidences.append(confidence)\n","            coordinates.append(coord)\n","\n","        return texts, confidences, coordinates\n","\n","    except UnidentifiedImageError:\n","        print(f\"[WARNING] Impossible d'identifier le fichier image. Ignorer...\")\n","        return [], [], []\n","    except Exception as e:\n","        print(f\"[ERROR] Erreur lors de l'application de l'OCR: {e}\")\n","        return [], [], []\n","\n","# Liste pour stocker les résultats\n","ocr_results = []\n","\n","# Charger les noms d'image déjà traités pour éviter les doublons\n","images_deja_traitees = set(df_ocr['image_ID'])\n","print(f\"Nombre d'images déjà traitées : {len(images_deja_traitees)}\")\n","\n","# Appliquer le prétraitement et l'OCR sur toutes les images filtrées après prétraitement avec multithreading\n","print(f\"Début de l'OCR sur {len(label_invoice_images)} images\")\n","with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","    futures = {}\n","    for image, label, image_ID in label_invoice_images:\n","        # ID de l'image\n","        image_name = image_ID\n","\n","        # Vérifier si l'image a déjà été traitée\n","        if image_name in images_deja_traitees:\n","            #print(f\"Image {image_name} déjà traitée, passage...\")\n","            continue\n","\n","        # Convertir l'image en bytes directement si ce n'est pas déjà le cas\n","        if isinstance(image, Image.Image):\n","            image_bytes = io.BytesIO()\n","            image.save(image_bytes, format='TIFF')\n","            image_bytes = image_bytes.getvalue()\n","        else:\n","            image_bytes = image\n","\n","        preprocessed_image_bytes = preprocess_image(image_bytes)\n","        if preprocessed_image_bytes:\n","            future = executor.submit(apply_ocr, preprocessed_image_bytes)\n","            futures[future] = (image_name, label, preprocessed_image_bytes)\n","\n","    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Prétraitement et OCR des images\", unit=\"image\"):\n","        image_name, label, preprocessed_image_bytes = futures[future]\n","        texts, confidences, coordinates = future.result()\n","\n","        # Enregistrer les résultats dans la liste\n","        ocr_results.append({\n","            'image_ID': image_name,\n","            'texte_de_ocr': \" \".join(texts),\n","            'confiance_de_ocr': np.mean(confidences),\n","            'label_de_image': label\n","        })\n","\n","        # Sauvegarder les résultats dans le DataFrame existant et enregistrer le fichier CSV\n","        new_row = pd.DataFrame([{\n","            'image_ID': image_name,\n","            'texte_de_ocr': \" \".join(texts),\n","            'confiance_de_ocr': np.mean(confidences),\n","            'label_de_image': label\n","        }])\n","        temp_file = os.path.join(temp_dir, f\"temp_{image_name}.csv\")\n","        new_row.to_csv(temp_file, header=False, index=False)\n","\n","print(\"Tous les résultats de l'OCR ont été enregistrés dans \"+ csv_path)\n","end_time = time.time()\n","processing_time = end_time - start_time\n","print(f\"Temps de traitement total : {round(processing_time / 60, 2)} minutes\")"]},{"cell_type":"markdown","source":["#réunion de tous les fichiers du train"],"metadata":{"id":"JfQvkq-EaJuw"}},{"cell_type":"code","source":["!cp \"/content/result/ocr_train08_02_dataset_ID.csv\" \"/content/drive/MyDrive/formation Datascientest/RVL-CDIP\""],"metadata":{"id":"nGSoARjjK44H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from tqdm import tqdm\n","\n","#-------------------------------------------------------------------------------\n","#paramétrage de lancement\n","#-------------------------------------------------------------------------------\n","#chemin pour trouver les datasets en enregistrer le csv\n","results_dir = '/content/result/'\n","# Nombre de threads pour le multithreading\n","num_threads = 40\n","temp_dir = '/content/tempreducedtrain/'\n","csv_path = results_dir +'ocr_train08_02_dataset_ID.csv'\n","\n","#-------------------------------------------------------------------------------\n","#-------------------------------------------------------------------------------\n","\n","# Vérifier si le répertoire results_dir existe et le créer s'il n'existe pas\n","if not os.path.exists(results_dir):\n","    os.makedirs(results_dir)\n","\n","# Liste de tous les fichiers CSV dans le répertoire\n","fichiers_csv = [f for f in os.listdir(temp_dir) if f.endswith('.csv')]\n","\n","# Nom du fichier de sortie\n","fichier_sortie = csv_path\n","\n","# Définir les en-têtes\n","colonnes = ['image_ID', 'texte_de_ocr', 'confiance_de_ocr', 'label_de_image']\n","\n","# Charger le fichier de sortie existant s'il existe\n","df_concatene = pd.DataFrame(columns=colonnes)\n","\n","# Traiter les fichiers par lots\n","taille_lot = 1000  # Nombre de fichiers à traiter par lot\n","\n","for i in tqdm(range(0, len(fichiers_csv), taille_lot), desc=\"Concaténation des fichiers CSV\"):\n","    dataframes = []\n","    for fichier in fichiers_csv[i:i + taille_lot]:\n","        chemin_fichier = os.path.join(temp_dir, fichier)\n","        df = pd.read_csv(chemin_fichier, header=None, names=colonnes, sep=',')\n","        dataframes.append(df)\n","\n","    # Concaténer les DataFrames du lot actuel\n","    df_lot = pd.concat(dataframes, ignore_index=True)\n","\n","    # Ajouter les données concaténées au DataFrame existant\n","    df_concatene = pd.concat([df_concatene, df_lot], ignore_index=True)\n","\n","    # Ajouter les données concaténées au fichier de sortie\n","    df_concatene.to_csv(fichier_sortie, header=True, index=False)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrQB7Ez6MSYW","executionInfo":{"status":"ok","timestamp":1726737279424,"user_tz":-120,"elapsed":15480,"user":{"displayName":"xav tru","userId":"01413854554802189079"}},"outputId":"7c7d2e61-8783-47e9-bf7e-06192e814b48","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Concaténation des fichiers CSV:   0%|          | 0/10 [00:00<?, ?it/s]<ipython-input-3-27738656af5f>:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  df_concatene = pd.concat([df_concatene, df_lot], ignore_index=True)\n","Concaténation des fichiers CSV: 100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n"]}]},{"cell_type":"markdown","source":["#nettoyage du trainocr 3 4 5"],"metadata":{"id":"oGvBOY0ac6wG"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import ast\n","\n","# Charger le fichier CSV existant\n","df = pd.read_csv('/content/drive/MyDrive/formation Datascientest/RVL-CDIP/ocr_train/pretraite/Ocr_train_image_IDs_batch_5.csv')\n","\n","\n","# Fonction pour convertir une chaîne de caractères représentant une liste en une vraie liste\n","def convertir_chaine_en_liste(chaine):\n","    try:\n","        # Convertir la chaîne de caractères en une liste Python\n","        return ast.literal_eval(chaine)\n","    except (ValueError, SyntaxError):\n","        # Si la conversion échoue, retourner NaN\n","        return np.nan\n","\n","# Appliquer la fonction pour convertir les chaînes en listes dans 'confiance_de_ocr' et 'texte_de_ocr'\n","df['confiance_de_ocr'] = df['confiance_de_ocr'].apply(convertir_chaine_en_liste)\n","df['texte_de_ocr'] = df['texte_de_ocr'].apply(convertir_chaine_en_liste)\n","\n","# Fonction pour transformer la liste texte_de_ocr en une seule chaîne de texte sans filtrage\n","def transformer_texte(liste_texte):\n","    if isinstance(liste_texte, list):\n","        # Concaténer tous les éléments de la liste en une seule chaîne sans filtrer les caractères\n","        texte_concatene = \" \".join(liste_texte)\n","        return texte_concatene\n","    return np.nan\n","\n","# Appliquer la fonction de transformation sur la colonne texte_de_ocr\n","df['texte_de_ocr'] = df['texte_de_ocr'].apply(transformer_texte)\n","\n","# Fonction pour calculer la moyenne des valeurs de confiance\n","def calculer_moyenne_confiance(confiances):\n","    try:\n","        # Calculer la moyenne si la liste est non vide et que c'est bien une liste\n","        return np.mean(confiances) if isinstance(confiances, list) and len(confiances) > 0 else np.nan\n","    except:\n","        # En cas d'erreur de calcul (par exemple si ce n'est pas une liste), retourner NaN\n","        return np.nan\n","\n","# Appliquer la fonction pour calculer la moyenne des valeurs de confiance\n","df['confiance_de_ocr'] = df['confiance_de_ocr'].apply(calculer_moyenne_confiance)\n","\n","# Ne conserver que les colonnes pertinentes (image_ID, texte_de_ocr, moyenne_confiance)\n","df_final = df[['image_ID', 'texte_de_ocr', 'confiance_de_ocr', 'label_de_image']]\n","\n","\n","# Sauvegarder dans un nouveau fichier CSV\n","df_final.to_csv('/content/drive/MyDrive/formation Datascientest/RVL-CDIP/ocr_train/Ocr_train5.csv', index=False)\n","\n","print(\"Transformation terminée et fichier sauvegardé sous 'fichier_output.csv'\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duyADrdqdAvU","executionInfo":{"status":"ok","timestamp":1726819798108,"user_tz":-120,"elapsed":17829,"user":{"displayName":"xav tru","userId":"01413854554802189079"}},"outputId":"a45386ca-6de8-491f-e2a4-fdcfd1fec790"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformation terminée et fichier sauvegardé sous 'fichier_output.csv'\n"]}]},{"cell_type":"markdown","source":["# Réunion des fichiers OCR image en un seul fichier"],"metadata":{"id":"4EwZ3UrnBRQy"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from tqdm import tqdm\n","\n","#-------------------------------------------------------------------------------\n","#paramétrage de lancement\n","#-------------------------------------------------------------------------------\n","#chemin pour trouver les datasets en enregistrer le csv\n","results_dir = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/'\n","# Nombre de threads pour le multithreading\n","num_threads = 40\n","temp_dir = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/ocr_train'\n","csv_path = results_dir +'ocr_train_dataset_ID_complet.csv'\n","\n","#-------------------------------------------------------------------------------\n","#-------------------------------------------------------------------------------\n","\n","# Vérifier si le répertoire results_dir existe et le créer s'il n'existe pas\n","if not os.path.exists(results_dir):\n","    os.makedirs(results_dir)\n","\n","# Liste de tous les fichiers CSV dans le répertoire\n","fichiers_csv = [f for f in os.listdir(temp_dir) if f.endswith('.csv')]\n","\n","# Nom du fichier de sortie\n","fichier_sortie = csv_path\n","\n","# Définir les en-têtes\n","colonnes = ['image_ID', 'texte_de_ocr', 'confiance_de_ocr', 'label_de_image']\n","\n","# Charger le fichier de sortie existant s'il existe\n","df_concatene = pd.DataFrame(columns=colonnes)\n","\n","# Traiter les fichiers par lots\n","taille_lot = 1000  # Nombre de fichiers à traiter par lot\n","\n","for i in tqdm(range(0, len(fichiers_csv), taille_lot), desc=\"Concaténation des fichiers CSV\"):\n","    dataframes = []\n","    for fichier in fichiers_csv[i:i + taille_lot]:\n","        chemin_fichier = os.path.join(temp_dir, fichier)\n","        df = pd.read_csv(chemin_fichier, header=None, names=colonnes, sep=',')\n","        dataframes.append(df)\n","\n","    # Concaténer les DataFrames du lot actuel\n","    df_lot = pd.concat(dataframes, ignore_index=True)\n","\n","    # Ajouter les données concaténées au DataFrame existant\n","    df_concatene = pd.concat([df_concatene, df_lot], ignore_index=True)\n","\n","    # Ajouter les données concaténées au fichier de sortie\n","    df_concatene.to_csv(fichier_sortie, header=True, index=False)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HjQ3-I8faIGl","executionInfo":{"status":"ok","timestamp":1726819869418,"user_tz":-120,"elapsed":15465,"user":{"displayName":"xav tru","userId":"01413854554802189079"}},"outputId":"6f09a530-04c3-4440-a09b-beba19b8fcd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Concaténation des fichiers CSV: 100%|██████████| 1/1 [00:15<00:00, 15.56s/it]\n"]}]},{"cell_type":"markdown","source":["#nettoyage fichier final, on enlève  les entete en trop"],"metadata":{"id":"WoWy1mG_hwzw"}},{"cell_type":"code","source":["import csv\n","\n","def nettoyer_csv(fichier_entree, fichier_sortie):\n","    # Ouvrir le fichier d'entrée en mode lecture\n","    with open(fichier_entree, 'r', newline='', encoding='utf-8') as entree:\n","        lecteur_csv = csv.reader(entree)\n","\n","        # Ouvrir le fichier de sortie en mode écriture\n","        with open(fichier_sortie, 'w', newline='', encoding='utf-8') as sortie:\n","            ecrivain_csv = csv.writer(sortie)\n","\n","            # Lire et écrire la première ligne (l'en-tête)\n","            en_tete = next(lecteur_csv)\n","            ecrivain_csv.writerow(en_tete)\n","\n","            # Parcourir le reste du fichier et écrire uniquement les lignes qui ne sont pas des en-têtes\n","            for ligne in lecteur_csv:\n","                # Si la ligne n'est pas identique à l'en-tête, l'écrire dans le fichier de sortie\n","                if ligne != en_tete:\n","                    ecrivain_csv.writerow(ligne)\n","\n","# Spécifier les fichiers d'entrée et de sortie\n","fichier_entree = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/ocr_train_dataset_ID_complet.csv'  # Remplace par le nom de ton fichier CSV\n","fichier_sortie = '/content/drive/MyDrive/formation Datascientest/RVL-CDIP/ocr_train_dataset_ID_propre.csv'     # Le nom du fichier de sortie\n","\n","# Appeler la fonction pour nettoyer le CSV\n","nettoyer_csv(fichier_entree, fichier_sortie)\n","\n","print(f\"Nettoyage terminé ! Le nouveau fichier est sauvegardé sous le nom {fichier_sortie}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JM0jpn7th2JL","executionInfo":{"status":"ok","timestamp":1726820690375,"user_tz":-120,"elapsed":14496,"user":{"displayName":"xav tru","userId":"01413854554802189079"}},"outputId":"0b072e0f-e70b-43be-fd4e-e2b017553070"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Nettoyage terminé ! Le nouveau fichier est sauvegardé sous le nom /content/drive/MyDrive/formation Datascientest/RVL-CDIP/ocr_train_dataset_ID_propre.csv.\n"]}]}]}